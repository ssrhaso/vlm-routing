{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a512158f",
   "metadata": {},
   "source": [
    "## CLIP LOADING AND INFERENCE TESTING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b47c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH VERSION: 2.9.1+cpu\n",
      "CUDA AVAILABLE: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "print(f\"TORCH VERSION: {torch.__version__}\")\n",
    "print(f\"CUDA AVAILABLE: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c110ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING CLIP MODEL\n",
      "CLIP MODEL LOADED IN 1.88 SECONDS\n",
      "MODEL DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "# LOAD CLIP MODEL\n",
    "\n",
    "print(\"LOADING CLIP MODEL\")\n",
    "start_time = time.time()\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"CLIP MODEL LOADED IN {load_time:.2f} SECONDS\")\n",
    "print(f\"MODEL DEVICE: {next(model.parameters()).device}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ff9cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING INFERENCE ON CLIP MODEL\n",
      "INFERENCE COMPLETED IN 0.12 SECONDS\n",
      "\n",
      "SCORES:\n",
      "'a photo of a cat': 0.9973\n",
      "'a photo of a dog': 0.0027\n",
      "'a photo of a car': 0.0001\n"
     ]
    }
   ],
   "source": [
    "# TEST CLIP INFERENCE ON SINGLE IMAGE-TEXT PAIR\n",
    "\n",
    "# DOWNLOAD IMAGE\n",
    "image_path = \"img/cat_image.jpg\" # EXAMPLE SINGLE IMAGE TAKEN FROM WIKIPEDIA (REAL LABEL = CAT)\n",
    "\n",
    "# LOAD IMAGE\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image.thumbnail((224, 224)) # RESIZE FOR SPEED \n",
    "\n",
    "# INFERENCE RUN\n",
    "\n",
    "print(\"RUNNING INFERENCE ON CLIP MODEL\")\n",
    "texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]  # EXAMPLE TEXTS\n",
    "inputs = processor(text = texts, images = image, return_tensors = \"pt\", padding = True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# TIME METRICS\n",
    "start = time.time()\n",
    "outputs = model(**inputs)\n",
    "inference_time = time.time() - start\n",
    "\n",
    "logits_per_image = outputs.logits_per_image  # THIS IS THE IMAGE-TO-TEXT SIMILARITY SCORE\n",
    "probs = logits_per_image.softmax(dim=1)  # WE CAN TAKE THE SOFTMAX TO GET THE PROBABILITIES OF EACH TEXT\n",
    "\n",
    "print(f\"INFERENCE COMPLETED IN {inference_time:.2f} SECONDS\")\n",
    "print(f\"\\nSCORES:\")\n",
    "for text, prob in zip(texts, probs[0]):\n",
    "    print(f\"'{text}': {prob.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838b757",
   "metadata": {},
   "source": [
    "## LLaVA LOADING AND INFERENCE TESTING:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcea9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING LLavA MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 30.87it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA MODEL LOADED IN 2.92 SECONDS\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "print(\"LOADING LLavA MODEL\")\n",
    "\n",
    "start_time = time.time()\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\",  # MODEL NAME\n",
    "    torch_dtype=torch.float16, # USE FP16 FOR SPEED\n",
    "    device_map=\"auto\", # AUTOMATICALLY PLACE ON GPU IF AVAILABLE\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"LLaVA MODEL LOADED IN {load_time:.2f} SECONDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e83691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING LLaVA INFERENCE (STANDARD AUTO-PROCESSOR)\n",
      "\n",
      "PROMPT: 'USER: <image>\n",
      "Describe this image in detail.\n",
      "ASSISTANT:'\n",
      "GENERATING RESPONSE...\n",
      "\n",
      " INFERENCE COMPLETED IN 2700.31 SECONDS\n",
      "\n",
      " LLaVA Response:\n",
      "USER:  \n",
      "Describe this image in detail.\n",
      "ASSISTANT: The image features a cat sitting on a branch, possibly a tree branch. The cat is looking at the camera, displaying a curious expression. The cat is positioned in the center of the image, with its body facing the viewer. The scene appears to be outdoors, with the cat enjoying its time in the natural environment.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import time\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "print(\"TESTING LLaVA INFERENCE (STANDARD AUTO-PROCESSOR)\")\n",
    "\n",
    "# LOAD IMAGE / PROCESSOR\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "# DEFINE PROMPT\n",
    "prompt = \"USER: <image>\\nDescribe this image in detail.\\nASSISTANT:\"\n",
    "\n",
    "# INPUTS\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# INFERENCE RUN\n",
    "print(f\"\\nPROMPT: '{prompt}'\")\n",
    "print(f\"GENERATING RESPONSE...\")\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False\n",
    "    )\n",
    "inference_time = time.time() - start\n",
    "\n",
    "# DECODE RESPONSE\n",
    "response = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\n INFERENCE COMPLETED IN {inference_time:.2f} SECONDS\")\n",
    "print(f\"\\n LLaVA Response:\\n{response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
